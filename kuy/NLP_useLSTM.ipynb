{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supposed the input is in df, the data of X_test=['อย่า ลืม ติดตาม ชม รายการ พิเศษ','ด่วน ลด แบบ จัดเต็ม ของแถม มากมาย']\n",
    "#keras embed method, will update with the tokenizer and weight file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file):\n",
    "    with open(file, \"r\") as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"./org_model_weights.h5\")\n",
    "    return model\n",
    "\n",
    "def preproces_df(csv):\n",
    "    #preprocessing the data from df \n",
    "    df_a = pd.read_csv(csv)\n",
    "    df_a['split_text'] = df_a.apply(lambda row: word_tokenize(row['tweet_text'],engine=\"newmm\",keep_whitespace=False), axis=1) #use this\n",
    "    df_a['combined'] = [' '.join(lst) for lst in df_a['split_text']]\n",
    "    df_a['cleaned'] = df_a['combined'].apply(deEmojify)\n",
    "    df_a['cleaned'] = df_a['cleaned'].apply(stopwords_rm) \n",
    "    df_a['split_cleaned'] = df_a.apply(lambda row: word_tokenize(row['cleaned'],engine=\"newmm\",keep_whitespace=False), axis=1)\n",
    "    df_a['word_length'] = df_a['split_cleaned'].str.len()\n",
    "    return df_a\n",
    "\n",
    "#loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#sub function below---------------------------------------------------------------------------------------------------\n",
    "def stopwords_rm(text):\n",
    "    stopwords = set(thai_stopwords())\n",
    "    stopwords.update([\"nan\", \"-\", \"_\", \"\", \" \", \"฿\" ,\"ค่ะ\", \"ครับ\", \"จ้า\"])  # Add more stopwords as needed\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    cleaned_text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    return cleaned_text\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "# Load model and weight\n",
    "def Main(df):\n",
    "    model = load_model(\"org_model_architecture.json\")\n",
    "    df = preproces_df(df)\n",
    "    print('predicting...')\n",
    "    MAX_SEQUENCE_LENGTH = 63 #training file 44\n",
    "    MAX_WORDS = 2500 #2000\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    tokenizer.fit_on_texts(df.cleaned.values)\n",
    "\n",
    "    X_test=[df['cleaned'].iloc[-1]]\n",
    "    X_test=tokenizer.texts_to_sequences(X_test)\n",
    "    X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(X_test)\n",
    "    org = ['public work (โยธา)', 'municipal office (เทศกิจ)' ,'police department']\n",
    "\n",
    "    print('result:', predictions)\n",
    "    print('classs predicted: ', org[int(predictions.argmax(axis=-1))])\n",
    "    print('confidence %: ', predictions[0][[int(predictions.argmax(axis=-1))]])\n",
    "    #for checking from ori excel\n",
    "    #print('real cat is : ',df['org'].iloc[-1])\n",
    "    if 'org' not in df.index:\n",
    "        df.loc['org'] = 0 \n",
    "\n",
    "    df['org'].iloc[-1] = org[int(predictions.argmax(axis=-1))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def Main_relevant(df):\n",
    "    model = load_model(\"relevant_model_architecture.json\")\n",
    "    df = preproces_df(df) #df['tweet_text'] เป็นstring ของ tweet \n",
    "    print('predicting...')\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 63 #same as training file\n",
    "    MAX_WORDS = 2500 # Update to match the number of unique words expected by the embedding layer\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    tokenizer.fit_on_texts(df.cleaned.values)\n",
    "\n",
    "    X_test=[df['cleaned'].iloc[-1]]\n",
    "    X_test=tokenizer.texts_to_sequences(X_test)\n",
    "    X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(X_test)\n",
    "    rev = ['not relevant','relevant']\n",
    "\n",
    "    print('result:', predictions)\n",
    "    print('classs predicted: ', rev[int(predictions.argmax(axis=-1))])\n",
    "    print('confidence %: ', predictions[0][[int(predictions.argmax(axis=-1))]])\n",
    "    #for checking\n",
    "    print('real cat is : ',df['rev'].iloc[-1])\n",
    "\n",
    "    df_drop = df #if not change then it is relevant\n",
    "\n",
    "    if int(predictions.argmax(axis=-1))== 0:\n",
    "        print('not relevant')\n",
    "        df_drop = df[-1] #store the dropped df\n",
    "        df = df[:-1] #drop the last one\n",
    "    return df_drop, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' embedding_2/embeddings:0': Shape mismatch.The variable shape (2500, 100), and the assigned value shape (2000, 100) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_drop, df_updated \u001b[38;5;241m=\u001b[39m \u001b[43mMain_relevant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplzwork.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#input the df location\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_drop[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m df_updated[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]: \u001b[38;5;66;03m#relevant\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m Main(df_updated)\n",
      "Cell \u001b[1;32mIn[26], line 82\u001b[0m, in \u001b[0;36mMain_relevant\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMain_relevant\u001b[39m(df):\n\u001b[1;32m---> 82\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelevant_model_architecture.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     df \u001b[38;5;241m=\u001b[39m preproces_df(df) \u001b[38;5;66;03m#df['tweet_text'] เป็นstring ของ tweet \u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicting...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      3\u001b[0m     loaded_model_json \u001b[38;5;241m=\u001b[39m json_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model_from_json(loaded_model_json)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./org_model_weights.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\mrbvb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\mrbvb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:4361\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4358\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[0;32m   4359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4360\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4361\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot assign value to variable ' embedding_2/embeddings:0': Shape mismatch.The variable shape (2500, 100), and the assigned value shape (2000, 100) are incompatible."
     ]
    }
   ],
   "source": [
    "df_drop, df_updated = Main_relevant(\"plzwork.csv\") #input the df location\n",
    "if df_drop[-1] == df_updated[-1]: #relevant\n",
    "    df = Main(df_updated)\n",
    "    print('relevant, updated')\n",
    "    print('org is : ', )\n",
    "else: \n",
    "    print('irrelevant, row deleted')\n",
    "    print('deleted row : ', df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                              1004\n",
       "username                                                    AitKanphong\n",
       "tweet_id                                            1701505503111819503\n",
       "tweet_text            เหล่าสิงห์มอเตอร์ไซด์สายเท่ทุกท่านครับ กทม จับ...\n",
       "org                                                                 NaN\n",
       "relevant                                                          False\n",
       "category                                                        ทางเท้า\n",
       "query                 (\"ทางเท้า\" OR \"ทางเดิน\" OR \"ฟุตบาท\") AND (\"กรุ...\n",
       "datetime_of_tweet                                    2023-09-12 7:58:19\n",
       "datetime_of_query                                   2023-11-16 14:52:12\n",
       "link                  https://twitter.com/AitKanphong/status/1701505...\n",
       "mentioned_location                                              มีนบุรี\n",
       "location                                                            NaN\n",
       "image                 ['https://pbs.twimg.com/media/F5z1289bgAAzKC0?...\n",
       "split_text            [เหล่า, สิงห์, มอเตอร์, ไซ, ด์, สาย, เท่, ทุกท...\n",
       "combined              เหล่า สิงห์ มอเตอร์ ไซ ด์ สาย เท่ ทุกท่าน ครับ...\n",
       "cleaned               สิงห์ มอเตอร์ ไซ ด์ สาย เท่ ทุกท่าน กทม อ. เอ ...\n",
       "split_cleaned         [สิงห์, มอเตอร์, ไซ, ด์, สาย, เท่, ทุกท่าน, กท...\n",
       "word_length                                                          38\n",
       "Name: 552, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X_test\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mอย่า ลืม ติดตาม ชม รายการ พิเศษ\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mด่วน ลด แบบ จัดเต็ม ของแถม มากมาย\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m X_test\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_test)\n\u001b[0;32m      3\u001b[0m X_test\u001b[38;5;241m=\u001b[39mpad_sequences(X_test, maxlen\u001b[38;5;241m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "X_test=['อย่า ลืม ติดตาม ชม รายการ พิเศษ','ด่วน ลด แบบ จัดเต็ม ของแถม มากมาย']\n",
    "X_test=tokenizer.texts_to_sequences(X_test)\n",
    "X_test=pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
